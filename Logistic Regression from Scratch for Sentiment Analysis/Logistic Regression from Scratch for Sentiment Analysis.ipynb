{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from Scratch for Sentiment Analysis\n",
    "\n",
    "\n",
    "### Implementation of Cost Function, Gradient descent and Stochastic gradient for multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "dataset_dir = 'Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "def preprocessing(directory, dataset):\n",
    "    print('Loading for', dataset)\n",
    "    print('Loading Lexicon...')\n",
    "    pos_words_lex = np.loadtxt(\"Dataset/positive-words.txt\", dtype='str') #numpy function to read text and seperate them by ,\n",
    "    neg_words_lex = np.loadtxt(\"Dataset/negative-words.txt\", dtype='str')\n",
    "    \n",
    "    print('Loading files...')\n",
    "    #os.walk is used to traverse files and get the abosulte path of the file\n",
    "    #the absolute path are later used to read the data and build dataset\n",
    "    for root, dirs, files in os.walk(os.path.abspath((os.path.join(dataset_dir, dataset,'pos')))): #for pos review\n",
    "        files_pos = [os.path.join(root, file) for file in files]\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.abspath((os.path.join(dataset_dir, dataset,'neg')))): #for neg review\n",
    "        files_neg = [os.path.join(root, file) for file in files]\n",
    "\n",
    "    #generate y based on neg and pos reiew files\n",
    "    y = np.concatenate([np.ones((len(files_pos),1)), np.zeros((len(files_neg),1))]) \n",
    "    print('Loading data from files...')\n",
    "\n",
    "    data_pos       = [np.loadtxt(i, dtype='str', encoding='utf-8', comments=\"!@#!@$5464\") for i in files_pos]\n",
    "    data_neg       = [np.loadtxt(i, dtype='str', encoding='utf-8', comments=\"!@#!@$5464\") for i in files_neg]\n",
    "\n",
    "    #concatenate the pos and neg reviews to 1 array\n",
    "    print('Concatenating the negative and positive reviews...')\n",
    "    data    = np.concatenate([data_pos, data_neg])\n",
    "\n",
    "    print('Cleaning data...')\n",
    "    for rev in data:\n",
    "        for idx, i in enumerate(rev):\n",
    "            rev[idx] =  i.replace(\")\",\"\").replace(\"(\", \"\").replace(\"/>\", \"\").replace(\".\",\"\")\n",
    "\n",
    "    print('Following are the words in both negative and positive lex')\n",
    "    set(neg_words_lex).intersection(set(pos_words_lex))\n",
    "\n",
    "    print('\\nCounting negative and positive words...')  \n",
    "    pos_count = np.zeros(len(data))\n",
    "    neg_count = np.zeros(len(data))\n",
    "    \n",
    "    #enumerate means iterate data in array with index , idx\n",
    "    for idx, review in enumerate(data):\n",
    "        for i in review:\n",
    "            if i in pos_words_lex:\n",
    "                pos_count[idx] += 1\n",
    "            elif i in neg_words_lex:\n",
    "                neg_count[idx] += 1    \n",
    "\n",
    "    print('Building dataset from features...')\n",
    "    x1 = pos_count\n",
    "    x2 = neg_count\n",
    "    x3 = [int(os.path.basename(i).split('_')[1][0]) for i in files_pos]+[int(os.path.basename(i).split('_')[1][0]) for i in files_neg]\n",
    "    x4 = [math.log(len(x)) for x in data] \n",
    "\n",
    "    x5 = [1 if 'no'in i else 0 for i in data]\n",
    "\n",
    "    x6 = np.zeros(len(data))\n",
    "    for idx, i in enumerate(data):\n",
    "        for j in i:\n",
    "            if \"!\" in j:\n",
    "                x6[idx] = 1\n",
    "            break\n",
    "    #make dataset in matrix for \n",
    "    #25000 x 6 matrix, each row represent each datapoint/review and every column is a feature\n",
    "    dataset = np.vstack([x1,x2,x3,x4,x5,x6]).T\n",
    "    \n",
    "    means_X = np.mean(dataset,axis=0)\n",
    "    sd_X = np.std(dataset,axis = 0)\n",
    "    for i in range (0,len(dataset)):\n",
    "        for j in range(0,6):\n",
    "            dataset[i][j]= (dataset[i][j]-means_X[j])/sd_X[j]\n",
    "\n",
    "    return dataset, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading for train\n",
      "Loading Lexicon...\n",
      "Loading files...\n",
      "Loading data from files...\n",
      "Concatenating the negative and positive reviews...\n",
      "Cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the words in both negative and positive lex\n",
      "\n",
      "Counting negative and positive words...\n",
      "Building dataset from features...\n",
      "Loading for test\n",
      "Loading Lexicon...\n",
      "Loading files...\n",
      "Loading data from files...\n",
      "Concatenating the negative and positive reviews...\n",
      "Cleaning data...\n",
      "Following are the words in both negative and positive lex\n",
      "\n",
      "Counting negative and positive words...\n",
      "Building dataset from features...\n"
     ]
    }
   ],
   "source": [
    "#calling preprocessing for train and test data\n",
    "train_data, y_train = preprocessing(dataset_dir, 'train')\n",
    "test_data, y_test  = preprocessing(dataset_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of columns in data 7\n",
      "no of rows in data 25000\n"
     ]
    }
   ],
   "source": [
    "# adding x0 so we can easily compute THETA0\n",
    "x0=np.ones((len(y_train),1) , dtype = float)\n",
    "x1=np.ones((len(y_test),1) , dtype = float)\n",
    "X=np.column_stack((x0,train_data))\n",
    "Xtest=np.column_stack((x1,test_data))\n",
    "print(\"no of columns in data\",len(X[0]))\n",
    "print(\"no of rows in data\",len(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid function\n",
    "def sigmoid(x):\n",
    "    z = 1/(1 + np.exp(-x))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict function\n",
    "def predict(X,W):\n",
    "    hx=np.dot(X,W)\n",
    "    return hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid\n",
    "\n",
    "$$ \\hat{y} = \\frac{e^{-(w.x+b)}}{1+ e^{-(w.x+b)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting values of predict into sigmoid\n",
    "def aftersig(X,W):\n",
    "    prob=sigmoid(predict(X,W))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Entropy(Loss Function for logistic Regression)\n",
    "\n",
    "$$L_{CE}(w,b) = -[ylog\\sigma(w.x+b) + (1-y) log(1-\\sigma(w.x+b))]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross entropy\n",
    "def cross_entropy(W,X,Y):\n",
    "    \n",
    "    m = np.size(Y) \n",
    "    mul=aftersig(X,W)\n",
    "\n",
    "    loss = [-((Y[i]*np.log(mul[i]))+((1-Y[i])*np.log(1-mul[i]))) for i in range(m)]\n",
    "   \n",
    "    final= sum(loss)/m\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paramenters need to be learned by Model are the $\\theta_j$ values. These are\n",
    "the values that will be adjusted to minimize cost $J(\\theta)$.\n",
    "One way to do this is to use the gradient descent algorithm. \n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} (\\sigma(w.x+b)-y)x_j  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent batch\n",
    "def gradientbatch(X, Y, alpha, n_epoch):\n",
    "\n",
    "    features= np.size(X,1) # no of total features\n",
    "    m= np.size(Y)  # number of training examples\n",
    "    \n",
    "\n",
    "    cost = list()  # list to store cost for every epoch\n",
    "    \n",
    "    thetas = np.zeros((7,1) , dtype = float)  #intiliazes thetas\n",
    "    \n",
    "    for epoch in range (n_epoch):\n",
    "        \n",
    "        sig = aftersig(X, thetas) #predict y\n",
    "        loss=sig-Y                #Calculate Loss\n",
    "        \n",
    "        for i in range(features):\n",
    "            \n",
    "            multiply = np.multiply(loss.T,X[:,i])\n",
    "            summation =  np.sum(multiply)\n",
    "            thetas[i] = thetas[i] - ((alpha/m)*summation)\n",
    "            \n",
    "        cost.append(cross_entropy(thetas,X, Y))\n",
    "       \n",
    "    return thetas, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights from batch [[ 0.05640649]\n",
      " [ 0.75009815]\n",
      " [-0.74175489]\n",
      " [ 1.09008397]\n",
      " [-0.05813787]\n",
      " [-0.22691874]\n",
      " [-0.01876693]]\n",
      "cost from batch [0.46402699]\n"
     ]
    }
   ],
   "source": [
    "#calling of batch\n",
    "n_epoch= 1500\n",
    "alpha=0.01\n",
    "thetas, cost = gradientbatch(X, y_train, alpha, n_epoch)\n",
    "print(\"weights from batch\",thetas)\n",
    "print(\"cost from batch\", cost[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient stochastic\n",
    "def gradientStochastic (X, Y, alpha, n_epoch):\n",
    "    \n",
    "    \n",
    "    features= np.size(X,1)\n",
    "    m= np.size(Y)  # number of training examples]\n",
    "    \n",
    "    cost = list()  # list to store cost for every epoch\n",
    "    \n",
    "    thetas = np.zeros((X.shape[1],1), dtype = float)\n",
    "    \n",
    "    for i in range (n_epoch):\n",
    "        \n",
    "        irand =  random.randint(0, (m-1)) # random no to pick data row\n",
    "        \n",
    "        sig = aftersig(X[irand], thetas) #predict Y\n",
    "        loss=sig-Y[irand]                # loss\n",
    "        \n",
    "        for i in range(features):\n",
    "            \n",
    "            multiply = np.multiply(loss,X[irand,i])\n",
    "            thetas[i] = thetas[i] - (alpha*multiply)\n",
    "        \n",
    "        cost.append(cross_entropy(thetas,X, Y))\n",
    "    \n",
    "    return thetas, cost\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights from stochastic [[ 0.11378163]\n",
      " [ 0.77413188]\n",
      " [-0.91446385]\n",
      " [ 1.03327624]\n",
      " [-0.10667004]\n",
      " [-0.19081466]\n",
      " [-0.06897519]]\n",
      "cost from stochastic [0.46317527]\n"
     ]
    }
   ],
   "source": [
    "#calling of stochastic\n",
    "no_iteration= 1500\n",
    "alpha=0.01\n",
    "thetasScro, costScro = gradientStochastic(X, y_train, alpha, no_iteration)\n",
    "print(\"weights from stochastic\",thetasScro)\n",
    "print(\"cost from stochastic\", costScro[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for testdata for batch\n",
    "hx=aftersig(Xtest,thetas)\n",
    "predictionB = [1 if x >0.5 else 0 for x in hx]\n",
    "#print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for testdata for stochastic\n",
    "hx=aftersig(Xtest,thetasScro)\n",
    "predictionS = [1 if x >0.5 else 0 for x in hx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evalution\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "            TP += 1\n",
    "        if y_pred[i]==1 and y_actual[i]==0:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "            TN += 1\n",
    "        if y_pred[i]==0 and y_actual[i]==1:\n",
    "            FN += 1\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Gradeint Results\n",
      "Confusion Matrix: [[ 8264  2036]\n",
      " [ 4236 10464]]\n",
      "Accuracy:  0.83712\n",
      "Precision:  0.8023300970873787\n",
      "Recall:  0.66112\n",
      "F1 Score:  0.7249122807017544\n"
     ]
    }
   ],
   "source": [
    "#Batch Gradeint Results\n",
    "TP,FP,TN,FN=perf_measure(y_test,predictionB)\n",
    "accuracy= (TN + TN)/(TP+TN+FP+FN)\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "confusionMatrix = np.array([[TP, FP], \n",
    "    [FN, TN]])\n",
    "\n",
    "print(\"Batch Gradeint Results\")\n",
    "print(\"Confusion Matrix:\",confusionMatrix)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1 Score: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocastic Gradient Results\n",
      "Confusion Matrix: [[8747 2526]\n",
      " [3753 9974]]\n",
      "Accuracy:  0.79792\n",
      "Precision:  0.7759247760134835\n",
      "Recall:  0.69976\n",
      "F1 Score:  0.7358768350649898\n"
     ]
    }
   ],
   "source": [
    "#Stocastic Gradient Results\n",
    "TP,FP,TN,FN=perf_measure(y_test,predictionS)\n",
    "accuracy= (TN + TN)/(TP+TN+FP+FN)\n",
    "precision=TP/(TP+FP)\n",
    "recall=TP/(TP+FN)\n",
    "f1=2*((precision*recall)/(precision+recall))\n",
    "confusionMatrix = np.array([[TP, FP], \n",
    "    [FN, TN]])\n",
    "print(\"Stocastic Gradient Results\")\n",
    "\n",
    "print(\"Confusion Matrix:\",confusionMatrix)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1 Score: \",f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
