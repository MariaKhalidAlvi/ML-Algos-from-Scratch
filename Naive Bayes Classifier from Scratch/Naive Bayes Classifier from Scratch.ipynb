{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from numpy import genfromtxt\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from numpy import errstate,isneginf,array\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "## For Tweets Multiclass Sentiment Analysis (Positive, Negitive, Neutral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfClass = 3  # three classes\n",
    "dataset_dir = r'Tweets.csv' #directory\n",
    "stopWords = ['the', 'at', 'of', 'this', 'hence', 'an', 'do', 'us', 'r', 'u', 'ill', 'aa', 'does', 'did', 'im', 'also', 'with', 'to', 'he', 'she', 'or', 'a', 'that', 'in', 'is', 'on', 'for', 'our', 'by', 'its', 'my', 'your', 'you', 'me', 'i', 'it', 'whom', 'who', 'only', 'are', 'am', 'into', 'were', 'was', 'and', 'so', 'has', 'have', 'been', 'had', 'be', 'ours', 'ourselves', 'same', 'while', 'which', 'where', 'how', 'having', 'doing', 'myself', 'but', 'off', 'further', 'herself', 'himself', 'there', 'these', 'they', 'we', \"you're\", \"you've\", \"you'll\", \"you'd\", 'yours', 'yourself', 'yourselves', 'him', 'his', \"she's\", 'her', 'hers', \"it's\", 'itself', 'them', 'their', 'theirs', 'themselves', 'what', \"that'll\", 'those', 'being', 'if', 'because', 'as', 'until', 'about', 'against', 'between', 'through', 'during', 'before', 'after', 'above', 'below', 'from', 'up', 'down', 'out', 'over', 'under', 'again', 'then', 'once', 'here', 'when', 'why', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'own', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical label to digits for processing\n",
    "def labelCovert(label):\n",
    "    if label == \"neutral\": return 0\n",
    "    if label  == 'positive': return 1       #replace each label with numeric value\n",
    "    elif label  == 'negative': return 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(sentence):\n",
    "    return sentence.translate(str.maketrans('', '', string.punctuation))  #remove puncutaiton from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji for data cleaning\n",
    "def removeEmojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDigitsFromWords(word):\n",
    "    return''.join([i for i in word if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return array of bag of words \n",
    "def bagOfWords(text, vocab):\n",
    "    #sentence_words = extract_words(sentence)\n",
    "    # frequency word count\n",
    "    bag = np.zeros(len(vocab))\n",
    "    for word in text:\n",
    "        for i,w in enumerate(vocab):    #prepare bag of words\n",
    "            if w == word: \n",
    "                bag[i] += 1\n",
    "                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "labels = []\n",
    "texts = []\n",
    "with open(dataset_dir,'r',  encoding='utf8') as file:     # read and seperate text and their labels\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        texts.append(row[1])\n",
    "\n",
    "labels = labels[1:]\n",
    "texts = texts[1:]\n",
    "\n",
    "labels = [labelCovert(label) for label in labels ]\n",
    "labels, texts = zip(*sorted(zip(labels, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data\n",
    "def cleanData(texts):\n",
    "    \n",
    "    for i,_ in enumerate(texts):\n",
    "        texts[i] = ' '.join(word for word in texts[i].split(' ') if not word.startswith('@')) #remove airline name\n",
    "        \n",
    "    texts = [str(np.char.lower(text)) for text in texts]  #lower case\n",
    "    texts = [remove_punctuation(text) for text in texts]  # remove puncuation\n",
    "    \n",
    "    for i,_ in enumerate(texts):\n",
    "        texts[i] = ' '.join(word for word in texts[i].split() if word not in stopWords) #remove stop words\n",
    "        texts[i] = re.sub(r\"http\\S+\", \"\", texts[i])  #remove URL\n",
    "    \n",
    "    texts = [removeEmojis(text) for text in texts]  # remove emojis\n",
    "    texts = [ re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text) for text in texts]  # remove digits\n",
    "        \n",
    "    return texts\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits data into train and test in stratified way\n",
    "def splitData(labels, texts , splitPercentage):   \n",
    "    \n",
    "    trainLabels =[]\n",
    "    testLabels = []\n",
    "    \n",
    "    trainTexts = []\n",
    "    testTexts = []\n",
    "    \n",
    "    neutralCount = labels.count(0)\n",
    "    positiveCount = labels.count(1)\n",
    "    negativeCount = labels.count(2)\n",
    "    \n",
    "    testNeutralCount = int(neutralCount * splitPercentage)\n",
    "    testPositiveCount = int(positiveCount * splitPercentage)\n",
    "    testNegativeCount = int(negativeCount * splitPercentage)\n",
    "\n",
    "    \n",
    "    testLabels = testLabels + list(labels[: (testNeutralCount)])\n",
    "    testTexts = testTexts + list(texts[: (testNeutralCount)])\n",
    "    \n",
    "    trainLabels = trainLabels + list(labels[testNeutralCount : (neutralCount )])\n",
    "    trainTexts = trainTexts +  list(texts[testNeutralCount : (neutralCount)])\n",
    "    \n",
    "    testPositiveCount =  neutralCount + testPositiveCount         \n",
    "\n",
    "    \n",
    "    testLabels = testLabels + list(labels[neutralCount : testPositiveCount])\n",
    "    testTexts = testTexts + list(texts[ neutralCount : testPositiveCount])\n",
    "    \n",
    "    trainLabels = trainLabels + list(labels[testPositiveCount : (neutralCount+positiveCount)])\n",
    "    trainTexts = trainTexts + list(texts[testPositiveCount : (neutralCount+positiveCount)])\n",
    "    \n",
    "    testNegativeCount =  neutralCount + positiveCount + testNegativeCount\n",
    "    \n",
    "    testLabels = testLabels + list(labels[(neutralCount+positiveCount) : testNegativeCount])\n",
    "    testTexts = testTexts + list(texts[ (neutralCount+positiveCount ) : testNegativeCount])\n",
    "    \n",
    "    trainLabels = trainLabels + list(labels[testNegativeCount:])\n",
    "    trainTexts = trainTexts + list(texts[testNegativeCount: ])\n",
    "    \n",
    "    return testLabels, testTexts ,trainLabels , trainTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLabels, testTexts ,trainLabels , trainTexts =  splitData(labels, texts , 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for bag of words\n",
    "def preProcessTrainTextData(textTrainData,labels):\n",
    "    \n",
    "    \n",
    "    trainTexts = cleanData(textTrainData)  # clean train tweets   \n",
    "    tokenizeTrainText = [text.split() for text in trainTexts] # tokenize train tweets\n",
    " \n",
    "    neutralText =  []\n",
    "    positiveText = []\n",
    "    negativeText = []\n",
    "    \n",
    "    for i,value in enumerate(labels):\n",
    "        if value == 0:\n",
    "            neutralText.append(tokenizeTrainText[i])\n",
    "        elif value == 1:\n",
    "            positiveText.append(tokenizeTrainText[i])\n",
    "        elif value == 2:\n",
    "            negativeText.append(tokenizeTrainText[i])\n",
    "\n",
    "    neutralVocab = []\n",
    "    posVocab = []\n",
    "    negVocab = []\n",
    "    \n",
    "    for text in neutralText:\n",
    "        neutralVocab = neutralVocab + text   # preparing list of all the  neutral words in train data\n",
    "    \n",
    "    for text in positiveText:\n",
    "        posVocab = posVocab + text   # preparing list of all the positive words in train data\n",
    "    \n",
    "    for text in negativeText:\n",
    "        negVocab = negVocab + text   # preparing list of all the negative words in train data\n",
    "    \n",
    "    neutralVocab = list(set(neutralVocab))\n",
    "    posVocab = list(set(posVocab))\n",
    "    negVocab = list(set(negVocab))\n",
    "    \n",
    "    \n",
    "    totalVocab = list(set(neutralVocab + posVocab + negVocab))\n",
    "\n",
    "    trainNeuDataList = [bagOfWords(text,totalVocab) for text in  neutralText]   #bag of words for newtral train data \n",
    "    trainNeuDataMatrix = np.vstack(trainNeuDataList)    # data Matrix for neutral train data\n",
    "    \n",
    "    trainPosDataList = [bagOfWords(text,totalVocab) for text in  positiveText]   #bag of words for positive train data \n",
    "    trainPosDataMatrix = np.vstack(trainPosDataList)    # data Matrix for Pos train data\n",
    "    \n",
    "    trainNegDataList = [bagOfWords(text,totalVocab) for text in  negativeText]   #bag of words for positive train data \n",
    "    trainNegDataMatrix = np.vstack(trainNegDataList)    # data Matrix for Pos train data\n",
    "\n",
    "    return  trainNeuDataMatrix, trainPosDataMatrix, trainNegDataMatrix, neutralVocab,  posVocab , negVocab , totalVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNeuDataMatrix, trainPosDataMatrix, trainNegDataMatrix, neutralVocab,  posVocab , negVocab , totalVocab = preProcessTrainTextData(trainTexts, trainLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N_c$ be the number of documents in our training data with class c and $N_{doc}$ be the total number of documents\n",
    "$$Prior = \\frac{N_c}{N_{doc}}$$\n",
    "\n",
    "$$Liklihood =\\hat{P}(w_{i}|c)= \\frac{count(w_{i}|c)+1}{\\sum_{w\\in V}count(w_{i}|c)+|V|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(trainNeuDataMatrix, trainPosDataMatrix, trainNegDataMatrix,neutralVocab, posVocab , negVocab, totalVocab, labels, classes = [0,1,2] ):\n",
    "    \n",
    "    totalClasses = len(labels)\n",
    "    prior = {}\n",
    "    likelihood = {}\n",
    "    \n",
    "    for cls in classes:\n",
    "        \n",
    "        classCount = labels.count(cls)\n",
    "        prior[cls]= classCount/totalClasses       # prior for each class\n",
    "        \n",
    "        if cls == 0:\n",
    "            totalSum =  trainNeuDataMatrix.sum()\n",
    "            totalSum = totalSum + len(totalVocab)\n",
    "            \n",
    "            for i, word in enumerate(totalVocab):\n",
    "                likelihood[word,cls] = (np.sum(trainNeuDataMatrix[:,i]) + 1)/totalSum     # likelihood for neutral class\n",
    "                \n",
    "        elif cls == 1:\n",
    "            totalSum =  trainPosDataMatrix.sum()\n",
    "            totalSum = totalSum + len(totalVocab)\n",
    "            \n",
    "            for i, word in enumerate(totalVocab):\n",
    "                likelihood[word,cls] = (np.sum(trainPosDataMatrix[:,i]) + 1)/totalSum     # likelihood for positive class\n",
    "                \n",
    "        elif cls == 2:\n",
    "            totalSum =  trainNegDataMatrix.sum()\n",
    "            totalSum = totalSum + len(totalVocab)\n",
    "                    \n",
    "            for i, word in enumerate(totalVocab):\n",
    "                likelihood[word,cls] = (np.sum(trainNegDataMatrix[:,i]) + 1)/totalSum    # likelihood for negative class\n",
    "         \n",
    "    return prior, likelihood  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior, likelihood =  trainNaiveBayes(trainNeuDataMatrix, trainPosDataMatrix, trainNegDataMatrix,neutralVocab, posVocab , negVocab, totalVocab, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peprocess test data for prediction\n",
    "def PreProcessTestData(testTexts):\n",
    "    \n",
    "    testTexts = cleanData(testTexts)  # clean test in tweets\n",
    "    tokenizeTestText = [text.split() for text in testTexts] # tokenize test tweets\n",
    "    \n",
    "    return tokenizeTestText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call test data preprocessing\n",
    "textData = PreProcessTestData(testTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each class c\n",
    "$$sum[c] = logprior[c]$$\n",
    "\n",
    "for each word of a text\n",
    "$$if word\\in V then \\;\\; sum[c] = sum[c]+ loglikelihood[word,c]$$\n",
    "\n",
    "$$\\hat{y} = argmax_c sum[c]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict label of text data by using Prior and liklihood calculated from train data\n",
    "def predictTestData(testData , totalVocab, prior, likelihood, classes = [0,1,2]):\n",
    "    \n",
    "    probabilities = []\n",
    "    probability = np.zeros((1, len(classes)))\n",
    "    predictedLabels =  np.zeros((len(testData),1))\n",
    "\n",
    "    for i, tweet in enumerate(testData):\n",
    "        summation = {}\n",
    "        probability.fill(0)\n",
    "        \n",
    "        for cls in classes:\n",
    "            summation[cls] = 1\n",
    "            for word in tweet:\n",
    "                if word in totalVocab:\n",
    "                    summation[cls] = summation[cls] * likelihood[word,cls]   # summing liklihood of a tweet\n",
    "            probability[:,cls] =  prior[cls] * summation[cls]             # calculating probability of a tweet for each class\n",
    "        \n",
    "        predictedLabels[i] = np.argmax(probability)     # gettings highest probablity for each tweet\n",
    "\n",
    "\n",
    "    return  list(predictedLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict label for test data\n",
    "pred_Y = predictTestData(textData, totalVocab, prior, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns precision\n",
    "def getPrecision(TP, FP):  \n",
    "    return (TP)/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns recall\n",
    "def getRecall(TP,FN):   \n",
    "    return (TP)/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns accuracy\n",
    "def getAccuracy(TP,  total):  \n",
    "    return (TP)/(total )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns F1 Score\n",
    "def getF1Score(precision,recall ):\n",
    "    return (2*precision*recall)/(precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for evalution\n",
    "def perf_measure(y_actual, y_pred):\n",
    "    TP_Neutral = 0\n",
    "    FP_Neutral = 0\n",
    "    TN_Neutral = 0\n",
    "    FN_Neutral = 0\n",
    "    \n",
    "    TP_Positive = 0\n",
    "    FP_Positive = 0\n",
    "    TN_Positive = 0\n",
    "    FN_Positive = 0\n",
    "    \n",
    "    TP_Negative = 0\n",
    "    FP_Negative = 0\n",
    "    TN_Negative = 0\n",
    "    FN_Negative = 0\n",
    "    \n",
    "\n",
    "    for i in range(len(y_pred)): \n",
    "        if y_actual[i]==y_pred[i]==0:\n",
    "            TP_Neutral  += 1\n",
    "        if y_actual[i]==y_pred[i]==1:\n",
    "            TP_Positive  += 1\n",
    "        if y_actual[i]==y_pred[i]==2:\n",
    "            TP_Negative += 1\n",
    "        \n",
    "        if  y_actual[i]==0 and y_pred[i]!=0:\n",
    "            FN_Neutral += 1\n",
    "        if y_actual[i] ==1 and y_pred[i]!=1:\n",
    "            FN_Positive += 1\n",
    "        if y_actual[i]==2 and y_pred[i]!=2:\n",
    "            FN_Negative += 1\n",
    "        \n",
    "        if  y_pred[i]==0 and y_actual[i]!=y_pred[i]:\n",
    "            FP_Neutral += 1\n",
    "        if  y_pred[i]==1 and y_actual[i]!=y_pred[i]:\n",
    "            FP_Positive += 1\n",
    "        if  y_pred[i]==2 and y_actual[i]!=y_pred[i]:\n",
    "            FP_Negative += 1\n",
    "    \n",
    "    \n",
    "    precisionNeutral = getPrecision(TP_Neutral,FP_Neutral)\n",
    "    precisionPositive = getPrecision(TP_Positive,FP_Positive)\n",
    "    precisionNegative = getPrecision(TP_Negative,FP_Negative)\n",
    "        \n",
    "    recallNeutral = getRecall(TP_Neutral,FN_Neutral)\n",
    "    recallPositive = getRecall(TP_Positive,FN_Positive)\n",
    "    recallNegative =   getRecall(TP_Negative,FN_Negative)\n",
    "        \n",
    "    #accuray =  getAccuracy((TP_Neutral+TP_Positive+TP_Negative), len(y_pred))\n",
    "    \n",
    "    f1Neutral = getF1Score(precisionNeutral,recallNeutral)\n",
    "    f1Positive = getF1Score(precisionPositive,recallPositive)\n",
    "    f1Negative = getF1Score(precisionNegative,recallNegative)\n",
    "    \n",
    "    macroAverageF1Score = (f1Neutral+f1Positive+f1Negative)/3\n",
    "    \n",
    "        \n",
    "    print(f\"Precision of Neutral:{precisionNeutral}\")\n",
    "    print(f\"Precision of Positive:{precisionPositive}\")\n",
    "    print(f\"Precision of Negative:{precisionNegative}\")\n",
    "        \n",
    "    print(f\"Recall of Neutral:{recallNeutral}\")\n",
    "    print(f\"Recall of Positive:{recallPositive}\")\n",
    "    print(f\"Recall of Negative:{recallNegative}\")\n",
    "        \n",
    "    #print(f\"Overall Accuracy:{accuray}\")\n",
    "    print(f\"Macro Average F1 Score:{macroAverageF1Score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Neutral:0.7310924369747899\n",
      "Precision of Positive:0.8306709265175719\n",
      "Precision of Negative:0.7494736842105263\n",
      "Recall of Neutral:0.28109854604200324\n",
      "Recall of Positive:0.5508474576271186\n",
      "Recall of Negative:0.9700272479564033\n",
      "Macro Average F1 Score:0.6380312536081728\n"
     ]
    }
   ],
   "source": [
    "# prints evaluation report\n",
    "perf_measure(testLabels,pred_Y)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
